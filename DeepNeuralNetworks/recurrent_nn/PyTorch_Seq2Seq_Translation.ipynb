{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6lcZokAzK1CK",
    "outputId": "1d30cd7e-a8ca-4369-c3d6-37aca9d09bfa"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EcrgydCHK1CR"
   },
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self,name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0:'SOS',1:'EOS'}\n",
    "        self.n_words = 2\n",
    "        \n",
    "    def addSentence(self,sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "            \n",
    "    def addWord(self,word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1dbOWWe-K1CV"
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lhj2r3TEK1CZ"
   },
   "outputs": [],
   "source": [
    "## Preparing data for training\n",
    "# say we want eng-fra, then reverse implies fra-eng\n",
    "def readLangFiles(lang1,lang2,file_base_dir=None,reverse=False,read_limit=20000):\n",
    "    if not file_base_dir:\n",
    "        raise ValueError('Enter valid base directory')\n",
    "    \n",
    "    fname = file_base_dir + '{}-{}.txt'.format(lang1,lang2)\n",
    "    with open(fname,'r',encoding='utf-8') as f:\n",
    "        lines = [[normalizeString(x) for x in p.strip().split('\\t')] for p in f.readlines()]\n",
    "    \n",
    "    random.shuffle(lines)\n",
    "    lines = lines[:read_limit]\n",
    "    if reverse:\n",
    "        lines = [[y,x] for x,y in lines]\n",
    "        inp_lang = Lang(lang2)\n",
    "        out_lang = Lang(lang1)\n",
    "    else:\n",
    "        inp_lang = Lang(lang1)\n",
    "        out_lang = Lang(lnag2)\n",
    "        \n",
    "    return inp_lang,out_lang,lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJQo0KYOK1Cc"
   },
   "outputs": [],
   "source": [
    "# Remember to add the full path\n",
    "base_path = 'seq2seq_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ic1DzDncK1Cg"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return (len(p[0].split(' ')) < MAX_LENGTH) and \\\n",
    "        (len(p[1].split(' ')) < MAX_LENGTH) and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "LiBBXVcBK1Ck",
    "outputId": "02ae5f46-3b2b-400e-d697-9296c50307b0"
   },
   "outputs": [],
   "source": [
    "def prepareData(lang1,lang2,reverse=False):\n",
    "    inp_lang,out_lang,pairs = readLangFiles(lang1,lang2,base_path,reverse,read_limit=-1)\n",
    "    print(\"Read %s sentence pairs\"%len(pairs))\n",
    "    filteredPairs = filterPairs(pairs)\n",
    "    print(\"Pairs remaining after filtering: %s\"%len(filteredPairs))\n",
    "    for p in filteredPairs:\n",
    "        inp_lang.addSentence(p[0])\n",
    "        out_lang.addSentence(p[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(inp_lang.name, inp_lang.n_words)\n",
    "    print(out_lang.name, out_lang.n_words)\n",
    "    return inp_lang, out_lang, filteredPairs\n",
    "\n",
    "inp_lang, out_lang, pairs = prepareData('eng', 'fra', True)\n",
    "from random import shuffle\n",
    "shuffle(pairs)\n",
    "print(pairs[0],type(pairs[0]))\n",
    "#print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AJarkv26K1Co"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self,inp_dim,hidden_dim):\n",
    "        super(EncoderRNN,self).__init__()\n",
    "        self.hidden_size = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(inp_dim,hidden_dim)\n",
    "        self.gru = nn.GRU(hidden_dim,hidden_dim)\n",
    "        \n",
    "    def forward(self,inp, hidden):\n",
    "        embedded = self.embedding(inp).view(1,1,-1)\n",
    "        output,hidden = self.gru(embedded, hidden)\n",
    "        return output,hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size) # why 3-d tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2-2TU1ChK1Cs"
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,output_size,hidden_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inp, hidden):\n",
    "        output = self.embedding(inp).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KcwsG8TQK1Cv"
   },
   "outputs": [],
   "source": [
    "# Prepared Training Data\n",
    "# Things required - input tensor with indices of each word in the sentence   \n",
    "def index2Tensor(lang,word):\n",
    "    return torch.tensor(lang.word2index[word])\n",
    "    \n",
    "def sentence2Tensor(lang,sentence):\n",
    "    words = sentence.split()\n",
    "    sentenceTensor = torch.zeros((len(words)+1,1),dtype=torch.long)\n",
    "    for i,word in enumerate(words):\n",
    "        sentenceTensor[i] = index2Tensor(lang,word)\n",
    "    sentenceTensor[-1] = torch.tensor(1) # for 'EOS'\n",
    "    return sentenceTensor\n",
    "    \n",
    "def prepareTensors(lang1,lang2,pairs):\n",
    "    input_tensor = sentence2Tensor(lang1,pairs[0])\n",
    "    target_tensor = sentence2Tensor(lang2,pairs[1])\n",
    "    return input_tensor,target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xr86QoH_-xYl"
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self,hidden_dim,output_dim,dropout_p=0.1,max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.output_dim,self.hidden_dim)\n",
    "        self.attn = nn.Linear(self.hidden_dim*2,max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_dim*2,self.hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_dim,self.hidden_dim)\n",
    "        self.out = nn.Linear(self.hidden_dim,self.output_dim)\n",
    "    \n",
    "    def forward(self,inp,hidden,encoder_outputs):\n",
    "        embedded = self.embedding(inp).view(1,1,-1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        attn_weights = F.softmax(self.attn(torch.cat(\n",
    "                  (embedded[0],hidden[0]),1)),dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                    encoder_outputs.unsqueeze(0))\n",
    "        out = torch.cat((embedded[0],attn_applied[0]),1)\n",
    "        out = self.attn_combine(out).unsqueeze(0)\n",
    "\n",
    "        out = F.relu(out)\n",
    "        out, hidden = self.gru(out, hidden)\n",
    "\n",
    "        out = F.log_softmax(self.out(out[0]), dim=1)\n",
    "        return out, hidden, attn_weights    \n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,1,self.hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X5UcQM5Zl2-N"
   },
   "outputs": [],
   "source": [
    "EOS_token = 1\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor,target_tensor,encoder,decoder,encoder_optimizer, \\\n",
    "          decoder_optimizer,criterion,max_length=MAX_LENGTH):\n",
    "  \n",
    "    target_length = target_tensor.size(0)\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size)#, device=device)\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    \n",
    "    for i,inp in enumerate(input_tensor):\n",
    "        # just a simple hack\n",
    "        if (input_tensor.shape[0] <= MAX_LENGTH):\n",
    "            encoder_out,encoder_hidden = encoder(inp,encoder_hidden)\n",
    "            encoder_outputs[i] = encoder_out[0,0]\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_input = torch.tensor([0])\n",
    "    \n",
    "    loss = 0\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item()/target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YZ5kvnSIqiyn"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JI71CMze-x2T"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "from random import randint\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.show()\n",
    "\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    #training_pairs = [prepareTensors(inp_lang,out_lang,random.choice(pairs))\n",
    "    #                  for i in range(n_iters)]\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    num_pairs = len(pairs)\n",
    "\n",
    "    for i in range(1, n_iters + 1):\n",
    "        index = randint(0,num_pairs-1)\n",
    "        input_tensor,target_tensor = prepareTensors(inp_lang, out_lang, pairs[index])\n",
    "        \n",
    "        loss = train(input_tensor, target_tensor, encoder, \\\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        \n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, i / n_iters),\n",
    "                                         i, i / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if i % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    \n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZdPOzfYqkNv",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(inp_lang.n_words, hidden_size)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, out_lang.n_words, dropout_p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the model \n",
    "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)\n",
    "torch.save(encoder1.state_dict(),os.getcwd()+'encoder.pt')\n",
    "torch.save(attn_decoder1.state_dict(),os.getcwd()+'attn_decoder.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reload model\n",
    "encoder_saved = EncoderRNN(inp_lang.n_words, hidden_size)\n",
    "encoder_saved.load_state_dict(torch.load(os.getcwd()+'encoder.pt'))\n",
    "attn_decoder_saved = AttnDecoderRNN(hidden_size, out_lang.n_words, dropout_p=0.1)\n",
    "attn_decoder_saved.load_state_dict(torch.load(os.getcwd()+'attn_decoder.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_decoder_saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_token = 1\n",
    "def evaluate(encoder,decoder,inp_sentence,max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = sentence2Tensor(inp_lang,inp_sentence)\n",
    "        input_length = input_tensor.shape[0]\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size)#, device=device)\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        for i,inp in enumerate(input_tensor):\n",
    "            # just a simple hack\n",
    "            encoder_out,encoder_hidden = encoder(inp,encoder_hidden)\n",
    "            encoder_outputs[i] += encoder_out[0,0]\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_input = torch.tensor([[0]])\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output,decoder_hidden,decoder_attention = decoder( \\\n",
    "                decoder_input,decoder_hidden,encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(out_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "    return decoded_words, decoder_attentions[:di + 1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = pairs[i]\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')\n",
    "        \n",
    "evaluateRandomly(encoder1, attn_decoder1, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K2uu6MgR-yAs"
   },
   "source": [
    "Function to check encoder-decoder without Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "quevmxqHK1Cz"
   },
   "outputs": [],
   "source": [
    "def checkEncoderDecoder(inp_pairs,hidden_size=256,learning_rate=1e-4):\n",
    "    \n",
    "    # Encoder operates on the French Language\n",
    "    encoderRnn = EncoderRNN(inp_lang.n_words,hidden_size)\n",
    "    encoder_optimizer = optim.SGD(encoderRnn.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Decoder operates on the English Language\n",
    "    decoderRnn = DecoderRNN(out_lang.n_words,hidden_size)\n",
    "    decoder_optimizer = optim.SGD(decoderRnn.parameters(), lr=learning_rate)\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "    total_loss = []\n",
    "    for pair in inp_pairs:\n",
    "        encoder_hidden = encoderRnn.initHidden()\n",
    "        inp_tensor,target_tensor = prepareTensors(inp_lang, out_lang, pair)\n",
    "        for inp in inp_tensor:\n",
    "            encoder_out,encoder_hidden = encoderRnn(inp,encoder_hidden)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_inp = torch.tensor([0])\n",
    "\n",
    "        loss = 0\n",
    "        # by default enforce teacher learning\n",
    "        for t in target_tensor:\n",
    "            decoder_out,decoder_hidden = decoderRnn(decoder_inp,decoder_hidden)\n",
    "            loss += criterion(decoder_out,t)\n",
    "            decoder_inp = t\n",
    "\n",
    "        total_loss.append(loss)\n",
    "        loss.backward()\n",
    "        \n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        \n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "    \n",
    "    plt.plot(total_loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "7Mcm8yxnK1C2",
    "outputId": "63218efc-0768-4ac1-ae43-beccc1578d6c"
   },
   "outputs": [],
   "source": [
    "checkEncoderDecoder(pairs[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x4OLZ9hsK1C6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "PyTorch-Seq2Seq-Translation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
